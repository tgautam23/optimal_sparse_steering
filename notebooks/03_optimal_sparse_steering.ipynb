{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimal-sparse-steering-title"
   },
   "source": [
    "# Optimal Sparse Steering via Convex Optimization\n",
    "\n",
    "**Core idea:** Instead of heuristically selecting SAE features for steering, we formulate the problem as a convex program that finds the *sparsest* feature perturbation to shift model behavior past a target margin.\n",
    "\n",
    "## Two Formulations\n",
    "\n",
    "**SOCP (Second-Order Cone Program):**\n",
    "$$\\min_{\\delta \\geq 0} \\; \\mathbf{1}^\\top \\delta \\quad \\text{s.t.} \\quad (Dw)^\\top \\delta \\geq \\tau' - w^\\top h - b, \\quad \\|D^\\top \\delta\\|_2 \\leq \\epsilon$$\n",
    "\n",
    "**QP (Quadratic Program) — relaxed formulation:**\n",
    "$$\\min_{\\delta \\geq 0} \\; \\mathbf{1}^\\top \\delta + \\frac{\\lambda}{2} \\|D^\\top \\delta\\|_2^2 \\quad \\text{s.t.} \\quad (Dw)^\\top \\delta \\geq \\tau' - w^\\top h - b$$\n",
    "\n",
    "Where $\\delta$ is the sparse feature perturbation, $D$ is the SAE decoder, $w$ is the probe weight vector, $h$ is the input's activation, and $\\tau'$ is the target margin.\n",
    "\n",
    "The QP moves the coherence (L2) constraint into the objective as a penalty, enabling faster QP solvers and warm-starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": "# @title Setup\nimport sys, os\n\nIN_COLAB = 'google.colab' in sys.modules\nif IN_COLAB:\n    REPO_DIR = '/content/optimal_sparse_steering'\n    if os.path.exists(REPO_DIR):\n        !git -C {REPO_DIR} pull -q\n    else:\n        !git clone -q https://github.com/tgautam23/optimal_sparse_steering.git {REPO_DIR}\n    !pip install -q torch transformer-lens sae-lens cvxpy datasets scikit-learn matplotlib seaborn tqdm transformers 2>&1 | grep -v \"dependency conflicts\\|incompatible\\|pip's dependency resolver\"\n    PROJECT_ROOT = REPO_DIR\n    try:\n        import numpy as _np; import seaborn as _sns; del _np, _sns\n    except (ValueError, ImportError):\n        print(\"Restarting runtime for numpy compatibility...\")\n        os.kill(os.getpid(), 9)\nelse:\n    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n\nsys.path.insert(0, PROJECT_ROOT)\n\nimport logging\nimport time\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom configs.base import ExperimentConfig, ModelConfig\nfrom src.models.wrapper import ModelWrapper\nfrom src.models.sae_utils import get_decoder_matrix\nfrom src.data.loader import load_dataset_splits\nfrom src.data.preprocessing import extract_activations, extract_sae_features\nfrom src.data.prompts import get_neutral_queries\nfrom src.probes.linear_probe import LinearProbe\nfrom src.steering.convex_optimal import ConvexOptimalSteering\nfrom src.steering.qp_optimal import QPOptimalSteering\nfrom src.evaluation.metrics import compute_probe_score, compute_l0\nfrom src.evaluation.generation import steered_generation\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')\nsns.set_theme(style=\"whitegrid\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpt2-sst2-header"
   },
   "source": [
    "---\n",
    "## GPT-2 Small + SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model-data-probe"
   },
   "outputs": [],
   "source": [
    "# @title Load Model + Data + Train Probe\n",
    "config = ExperimentConfig()\n",
    "config.model.device = device\n",
    "\n",
    "model_wrapper = ModelWrapper(config.model)\n",
    "layer = config.model.steering_layer\n",
    "\n",
    "# Load SAE + decoder matrix\n",
    "sae = model_wrapper.get_sae(layer)\n",
    "D = get_decoder_matrix(sae)  # (d_sae, d_model)\n",
    "d_sae, d_model = D.shape\n",
    "print(f\"Model: {config.model.name}, d_model: {d_model}\")\n",
    "print(f\"SAE: d_sae = {d_sae}, layer = {layer}\")\n",
    "\n",
    "# Load data\n",
    "data = load_dataset_splits(config.data)\n",
    "print(f\"Train: {len(data['train_texts'])}, Test: {len(data['test_texts'])}\")\n",
    "\n",
    "# Extract activations + SAE features\n",
    "train_acts = extract_activations(data['train_texts'], model_wrapper, layer,\n",
    "                                  batch_size=config.model.batch_size)\n",
    "test_acts = extract_activations(data['test_texts'], model_wrapper, layer,\n",
    "                                 batch_size=config.model.batch_size)\n",
    "test_sae_feats = extract_sae_features(data['test_texts'], model_wrapper, layer,\n",
    "                                       batch_size=config.model.batch_size)\n",
    "\n",
    "train_acts_np = train_acts.numpy()\n",
    "test_acts_np = test_acts.numpy()\n",
    "train_labels = np.array(data['train_labels'])\n",
    "test_labels = np.array(data['test_labels'])\n",
    "\n",
    "# Train probe\n",
    "probe = LinearProbe(d_model=d_model)\n",
    "probe.fit(train_acts_np, train_labels)\n",
    "print(f\"Probe accuracy \\u2014 train: {probe.score(train_acts_np, train_labels):.4f}, \"\n",
    "      f\"test: {probe.score(test_acts_np, test_labels):.4f}\")\n",
    "\n",
    "probe_w = probe.weight_vector\n",
    "probe_b = probe.bias\n",
    "target_class = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "socp-header"
   },
   "source": [
    "### SOCP Formulation\n",
    "The original formulation with a hard L2 coherence constraint $\\|D^\\top \\delta\\|_2 \\leq \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "socp-single"
   },
   "outputs": [],
   "source": [
    "# @title SOCP — Single Example Solve\n",
    "socp = ConvexOptimalSteering(epsilon=5.0, tau=0.5, solver=\"SCS\", prefilter_threshold=0.01)\n",
    "\n",
    "# Solve for the first test input\n",
    "h_0 = torch.tensor(test_acts_np[0])\n",
    "sae_feat_0 = test_sae_feats[0]\n",
    "\n",
    "t0 = time.time()\n",
    "sv_socp = socp.compute_steering_vector(\n",
    "    h=h_0, probe_w=probe_w, probe_b=probe_b,\n",
    "    D=D, sae_features=sae_feat_0, target_class=target_class,\n",
    ")\n",
    "socp_time = time.time() - t0\n",
    "\n",
    "print(f\"SOCP solve time: {socp_time:.3f}s\")\n",
    "print(f\"Status: {socp.solve_status}\")\n",
    "print(f\"Steering vector norm: {sv_socp.norm():.4f}\")\n",
    "print(f\"L0 (nonzero features): {compute_l0(socp.delta)}\")\n",
    "print(f\"L1 (total perturbation): {socp.delta.sum():.4f}\")\n",
    "print(f\"Active features: {socp.active_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp-header"
   },
   "source": [
    "### QP Formulation\n",
    "The relaxed formulation with a quadratic coherence penalty $\\frac{\\lambda}{2}\\|D^\\top \\delta\\|_2^2$ in the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp-single"
   },
   "outputs": [],
   "source": [
    "# @title QP — Single Example Solve\n",
    "qp = QPOptimalSteering(lam=1.0, tau=0.5, solver=\"SCS\", prefilter_topk=2000)\n",
    "\n",
    "t0 = time.time()\n",
    "sv_qp = qp.compute_steering_vector(\n",
    "    h=h_0, probe_w=probe_w, probe_b=probe_b,\n",
    "    D=D, sae_features=sae_feat_0, target_class=target_class,\n",
    ")\n",
    "qp_time = time.time() - t0\n",
    "\n",
    "print(f\"QP solve time: {qp_time:.3f}s\")\n",
    "print(f\"Status: {qp.solve_status}\")\n",
    "print(f\"Steering vector norm: {sv_qp.norm():.4f}\")\n",
    "print(f\"L0 (nonzero features): {compute_l0(qp.delta)}\")\n",
    "print(f\"L1 (total perturbation): {qp.delta.sum():.4f}\")\n",
    "print(f\"Active features: {qp.active_features}\")\n",
    "\n",
    "# Compare SOCP vs QP\n",
    "cos_sim = torch.nn.functional.cosine_similarity(sv_socp.unsqueeze(0), sv_qp.unsqueeze(0)).item()\n",
    "print(f\"\\nSOCP vs QP cosine similarity: {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sparsity-analysis-header"
   },
   "source": [
    "### Sparsity Pattern Analysis\n",
    "Which features does the optimizer select, and how do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sparsity-analysis"
   },
   "outputs": [],
   "source": [
    "# @title Sparsity Pattern Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# SOCP sparsity pattern\n",
    "socp_delta = socp.delta\n",
    "socp_nonzero = np.where(socp_delta > 1e-6)[0]\n",
    "ax = axes[0]\n",
    "ax.bar(range(len(socp_nonzero)), socp_delta[socp_nonzero], color='#3498db')\n",
    "ax.set_xticks(range(len(socp_nonzero)))\n",
    "ax.set_xticklabels([str(i) for i in socp_nonzero], rotation=45, fontsize=8)\n",
    "ax.set_xlabel('Feature Index')\n",
    "ax.set_ylabel('Perturbation Magnitude')\n",
    "ax.set_title(f'SOCP: {len(socp_nonzero)} active features')\n",
    "\n",
    "# QP sparsity pattern\n",
    "qp_delta = qp.delta\n",
    "qp_nonzero = np.where(qp_delta > 1e-6)[0]\n",
    "ax = axes[1]\n",
    "ax.bar(range(len(qp_nonzero)), qp_delta[qp_nonzero], color='#e74c3c')\n",
    "ax.set_xticks(range(len(qp_nonzero)))\n",
    "ax.set_xticklabels([str(i) for i in qp_nonzero], rotation=45, fontsize=8)\n",
    "ax.set_xlabel('Feature Index')\n",
    "ax.set_ylabel('Perturbation Magnitude')\n",
    "ax.set_title(f'QP: {len(qp_nonzero)} active features')\n",
    "\n",
    "plt.suptitle('Optimal Sparse Solutions: Feature Perturbation Patterns', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overlap analysis\n",
    "socp_set = set(socp_nonzero.tolist())\n",
    "qp_set = set(qp_nonzero.tolist())\n",
    "overlap = socp_set & qp_set\n",
    "print(f\"SOCP active features: {len(socp_set)}\")\n",
    "print(f\"QP active features:   {len(qp_set)}\")\n",
    "print(f\"Overlap:              {len(overlap)}\")\n",
    "if socp_set | qp_set:\n",
    "    print(f\"Jaccard similarity:   {len(overlap) / len(socp_set | qp_set):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch-header"
   },
   "source": [
    "### Batch Solving with Warm-Starting\n",
    "The QP supports warm-starting: the solution for input $i$ seeds the solve for input $i+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch-solve"
   },
   "outputs": [],
   "source": [
    "# @title Batch Solve: SOCP vs QP (with warm-start)\n",
    "n_test = min(20, len(data['test_texts']))\n",
    "\n",
    "# SOCP batch\n",
    "socp_batch = ConvexOptimalSteering(epsilon=5.0, tau=0.5, solver=\"SCS\")\n",
    "t0 = time.time()\n",
    "socp_vecs = socp_batch.compute_batch_steering(\n",
    "    activations=test_acts[:n_test],\n",
    "    probe_w=probe_w, probe_b=probe_b, D=D,\n",
    "    sae_features_batch=test_sae_feats[:n_test],\n",
    "    target_class=target_class,\n",
    ")\n",
    "socp_batch_time = time.time() - t0\n",
    "\n",
    "# QP batch (with warm-starting)\n",
    "qp_batch = QPOptimalSteering(lam=1.0, tau=0.5, solver=\"SCS\", warm_start=True)\n",
    "t0 = time.time()\n",
    "qp_vecs = qp_batch.compute_batch_steering(\n",
    "    activations=test_acts[:n_test],\n",
    "    probe_w=probe_w, probe_b=probe_b, D=D,\n",
    "    sae_features_batch=test_sae_feats[:n_test],\n",
    "    target_class=target_class,\n",
    ")\n",
    "qp_batch_time = time.time() - t0\n",
    "\n",
    "# QP without warm-starting\n",
    "qp_cold = QPOptimalSteering(lam=1.0, tau=0.5, solver=\"SCS\", warm_start=False)\n",
    "t0 = time.time()\n",
    "qp_cold_vecs = qp_cold.compute_batch_steering(\n",
    "    activations=test_acts[:n_test],\n",
    "    probe_w=probe_w, probe_b=probe_b, D=D,\n",
    "    sae_features_batch=test_sae_feats[:n_test],\n",
    "    target_class=target_class,\n",
    ")\n",
    "qp_cold_time = time.time() - t0\n",
    "\n",
    "print(f\"Batch of {n_test} solves:\")\n",
    "print(f\"  SOCP:              {socp_batch_time:.2f}s ({socp_batch_time/n_test:.3f}s avg)\")\n",
    "print(f\"  QP (warm-start):   {qp_batch_time:.2f}s ({qp_batch_time/n_test:.3f}s avg)\")\n",
    "print(f\"  QP (cold):         {qp_cold_time:.2f}s ({qp_cold_time/n_test:.3f}s avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shared-header"
   },
   "source": [
    "### QP Shared Steering Vector\n",
    "Instead of per-input solves, compute a single steering vector for the worst-case (hardest-to-steer) input in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shared-steering"
   },
   "outputs": [],
   "source": [
    "# @title Shared Steering Vector (Worst-Case)\n",
    "qp_shared = QPOptimalSteering(lam=1.0, tau=0.5, solver=\"SCS\")\n",
    "\n",
    "t0 = time.time()\n",
    "sv_shared = qp_shared.compute_shared_steering(\n",
    "    activations=test_acts[:n_test],\n",
    "    probe_w=probe_w, probe_b=probe_b, D=D,\n",
    "    sae_features=test_sae_feats[:n_test],\n",
    "    target_class=target_class,\n",
    ")\n",
    "shared_time = time.time() - t0\n",
    "\n",
    "print(f\"Shared solve time: {shared_time:.3f}s\")\n",
    "print(f\"L0: {compute_l0(qp_shared.delta)}\")\n",
    "print(f\"Steering norm: {sv_shared.norm():.4f}\")\n",
    "\n",
    "# Evaluate shared vector across all test inputs\n",
    "sv_shared_np = sv_shared.numpy()\n",
    "for alpha in [1.0, 3.0, 5.0, 10.0]:\n",
    "    steered = test_acts_np[:n_test] + alpha * sv_shared_np[np.newaxis, :]\n",
    "    base_score = compute_probe_score(probe, test_acts_np[:n_test], target_class)\n",
    "    steered_score = compute_probe_score(probe, steered, target_class)\n",
    "    print(f\"  alpha={alpha:5.1f}  probe_delta={steered_score - base_score:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pareto-header"
   },
   "source": [
    "### Pareto Sweep: Sparsity vs Effectiveness\n",
    "Sweep over $\\lambda$ (QP) and $\\epsilon$ (SOCP) to map the tradeoff between sparsity and steering success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pareto-sweep"
   },
   "outputs": [],
   "source": [
    "# @title Pareto Sweep over lambda (QP) and epsilon (SOCP)\n",
    "# Use a single test input for speed\n",
    "h_test = torch.tensor(test_acts_np[0])\n",
    "sae_test = test_sae_feats[0]\n",
    "\n",
    "# QP lambda sweep\n",
    "lam_values = [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 50.0]\n",
    "qp_pareto = []\n",
    "for lam in lam_values:\n",
    "    qp_i = QPOptimalSteering(lam=lam, tau=0.5, solver=\"SCS\", prefilter_topk=2000)\n",
    "    sv = qp_i.compute_steering_vector(\n",
    "        h=h_test, probe_w=probe_w, probe_b=probe_b,\n",
    "        D=D, sae_features=sae_test, target_class=target_class,\n",
    "    )\n",
    "    if qp_i.solve_status in (\"optimal\", \"optimal_inaccurate\"):\n",
    "        steered = test_acts_np[0:1] + 5.0 * sv.numpy()[np.newaxis, :]\n",
    "        delta = compute_probe_score(probe, steered, target_class) - \\\n",
    "                compute_probe_score(probe, test_acts_np[0:1], target_class)\n",
    "        qp_pareto.append({\n",
    "            'lam': lam, 'l0': compute_l0(qp_i.delta),\n",
    "            'l1': float(qp_i.delta.sum()),\n",
    "            'probe_delta': delta,\n",
    "            'coherence': float(np.linalg.norm((D.numpy().T @ qp_i.delta))),\n",
    "            'solve_time': qp_i.solve_time,\n",
    "        })\n",
    "        print(f\"lam={lam:6.2f}  L0={qp_pareto[-1]['l0']:4d}  \"\n",
    "              f\"probe_delta={delta:+.4f}  \"\n",
    "              f\"coherence={qp_pareto[-1]['coherence']:.3f}  \"\n",
    "              f\"time={qp_pareto[-1]['solve_time']:.3f}s\")\n",
    "\n",
    "# SOCP epsilon sweep\n",
    "eps_values = [1.0, 2.0, 3.0, 5.0, 10.0, 20.0, 50.0]\n",
    "socp_pareto = []\n",
    "for eps in eps_values:\n",
    "    socp_i = ConvexOptimalSteering(epsilon=eps, tau=0.5, solver=\"SCS\")\n",
    "    sv = socp_i.compute_steering_vector(\n",
    "        h=h_test, probe_w=probe_w, probe_b=probe_b,\n",
    "        D=D, sae_features=sae_test, target_class=target_class,\n",
    "    )\n",
    "    if socp_i.solve_status in (\"optimal\", \"optimal_inaccurate\"):\n",
    "        steered = test_acts_np[0:1] + 5.0 * sv.numpy()[np.newaxis, :]\n",
    "        delta = compute_probe_score(probe, steered, target_class) - \\\n",
    "                compute_probe_score(probe, test_acts_np[0:1], target_class)\n",
    "        socp_pareto.append({\n",
    "            'epsilon': eps, 'l0': compute_l0(socp_i.delta),\n",
    "            'l1': float(socp_i.delta.sum()),\n",
    "            'probe_delta': delta,\n",
    "            'coherence': float(np.linalg.norm((D.numpy().T @ socp_i.delta))),\n",
    "        })\n",
    "        print(f\"eps={eps:5.1f}  L0={socp_pareto[-1]['l0']:4d}  \"\n",
    "              f\"probe_delta={delta:+.4f}  \"\n",
    "              f\"coherence={socp_pareto[-1]['coherence']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pareto-plot"
   },
   "outputs": [],
   "source": [
    "# @title Pareto Frontier Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# L0 vs Probe Delta\n",
    "ax = axes[0]\n",
    "if qp_pareto:\n",
    "    ax.scatter([r['l0'] for r in qp_pareto],\n",
    "               [r['probe_delta'] for r in qp_pareto],\n",
    "               c='#e74c3c', s=80, zorder=3, label='QP (lambda sweep)')\n",
    "    ax.plot([r['l0'] for r in qp_pareto],\n",
    "            [r['probe_delta'] for r in qp_pareto],\n",
    "            color='#e74c3c', alpha=0.4)\n",
    "if socp_pareto:\n",
    "    ax.scatter([r['l0'] for r in socp_pareto],\n",
    "               [r['probe_delta'] for r in socp_pareto],\n",
    "               c='#3498db', s=80, marker='s', zorder=3, label='SOCP (epsilon sweep)')\n",
    "    ax.plot([r['l0'] for r in socp_pareto],\n",
    "            [r['probe_delta'] for r in socp_pareto],\n",
    "            color='#3498db', alpha=0.4)\n",
    "ax.set_xlabel('L0 (number of active features)')\n",
    "ax.set_ylabel('Probe Score Delta (alpha=5)')\n",
    "ax.set_title('Pareto Frontier: Sparsity vs Effectiveness')\n",
    "ax.legend()\n",
    "\n",
    "# Coherence vs Probe Delta\n",
    "ax = axes[1]\n",
    "if qp_pareto:\n",
    "    ax.scatter([r['coherence'] for r in qp_pareto],\n",
    "               [r['probe_delta'] for r in qp_pareto],\n",
    "               c='#e74c3c', s=80, zorder=3, label='QP')\n",
    "    for r in qp_pareto:\n",
    "        ax.annotate(f\"\\u03bb={r['lam']}\", (r['coherence'], r['probe_delta']),\n",
    "                    fontsize=7, alpha=0.7)\n",
    "if socp_pareto:\n",
    "    ax.scatter([r['coherence'] for r in socp_pareto],\n",
    "               [r['probe_delta'] for r in socp_pareto],\n",
    "               c='#3498db', s=80, marker='s', zorder=3, label='SOCP')\n",
    "    for r in socp_pareto:\n",
    "        ax.annotate(f\"\\u03b5={r['epsilon']}\", (r['coherence'], r['probe_delta']),\n",
    "                    fontsize=7, alpha=0.7)\n",
    "ax.set_xlabel('Coherence (||D^T delta||_2)')\n",
    "ax.set_ylabel('Probe Score Delta (alpha=5)')\n",
    "ax.set_title('Coherence vs Effectiveness')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation-header"
   },
   "source": [
    "### Generated Text Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steered-generation"
   },
   "outputs": [],
   "source": [
    "# @title Steered Generation — QP Optimal\n",
    "queries = get_neutral_queries(\"sst2\")[:5]\n",
    "\n",
    "# Solve QP for shared steering vector across test set\n",
    "qp_gen = QPOptimalSteering(lam=1.0, tau=0.5, solver=\"SCS\")\n",
    "sv_gen = qp_gen.compute_shared_steering(\n",
    "    activations=test_acts[:20],\n",
    "    probe_w=probe_w, probe_b=probe_b, D=D,\n",
    "    sae_features=test_sae_feats[:20],\n",
    "    target_class=target_class,\n",
    ")\n",
    "\n",
    "print(f\"QP steering: L0={compute_l0(qp_gen.delta)}, norm={sv_gen.norm():.4f}\")\n",
    "print()\n",
    "\n",
    "for alpha in [0.0, 5.0, 10.0]:\n",
    "    print(f\"--- alpha = {alpha} ---\")\n",
    "    try:\n",
    "        gens = steered_generation(model_wrapper, queries, qp_gen, layer,\n",
    "                                   alpha=alpha, max_new_tokens=50, temperature=0.7)\n",
    "        for i, g in enumerate(gens):\n",
    "            print(f\"  [{i+1}] {g[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gemma-header"
   },
   "source": [
    "---\n",
    "## Gemma-2-2B Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gemma-setup"
   },
   "outputs": [],
   "source": [
    "# @title Load Gemma-2-2B\n",
    "del model_wrapper\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gemma_config = ExperimentConfig(\n",
    "    model=ModelConfig(\n",
    "        name=\"gemma-2-2b-pt\",\n",
    "        tl_name=\"google/gemma-2-2b\",\n",
    "        sae_release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_16k/canonical\",\n",
    "        hook_template=\"blocks.{layer}.hook_resid_post\",\n",
    "        d_model=2304, n_layers=26, steering_layer=15,\n",
    "        dtype=\"float16\", device=device, batch_size=4,\n",
    "    ),\n",
    "    experiment_name=\"gemma-2-2b-pt_sst2\",\n",
    ")\n",
    "\n",
    "mw_g = ModelWrapper(gemma_config.model)\n",
    "layer_g = gemma_config.model.steering_layer\n",
    "sae_g = mw_g.get_sae(layer_g)\n",
    "D_g = get_decoder_matrix(sae_g)\n",
    "print(f\"Gemma d_sae: {D_g.shape[0]}, d_model: {D_g.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gemma-data-probe"
   },
   "outputs": [],
   "source": [
    "# @title Gemma: Data + Probe\n",
    "data_g = load_dataset_splits(gemma_config.data)\n",
    "\n",
    "train_acts_g = extract_activations(data_g['train_texts'], mw_g, layer_g, batch_size=4)\n",
    "test_acts_g = extract_activations(data_g['test_texts'], mw_g, layer_g, batch_size=4)\n",
    "test_sae_g = extract_sae_features(data_g['test_texts'], mw_g, layer_g, batch_size=4)\n",
    "\n",
    "train_g_np = train_acts_g.numpy()\n",
    "test_g_np = test_acts_g.numpy()\n",
    "train_labels_g = np.array(data_g['train_labels'])\n",
    "test_labels_g = np.array(data_g['test_labels'])\n",
    "\n",
    "probe_g = LinearProbe(d_model=2304)\n",
    "probe_g.fit(train_g_np, train_labels_g)\n",
    "print(f\"Probe \\u2014 train: {probe_g.score(train_g_np, train_labels_g):.4f}, \"\n",
    "      f\"test: {probe_g.score(test_g_np, test_labels_g):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gemma-qp"
   },
   "outputs": [],
   "source": [
    "# @title Gemma: QP Optimal Steering\n",
    "qp_g = QPOptimalSteering(lam=1.0, tau=0.5, solver=\"SCS\", prefilter_topk=2000)\n",
    "\n",
    "h_g = torch.tensor(test_g_np[0])\n",
    "sv_g = qp_g.compute_steering_vector(\n",
    "    h=h_g, probe_w=probe_g.weight_vector, probe_b=probe_g.bias,\n",
    "    D=D_g, sae_features=test_sae_g[0], target_class=1,\n",
    ")\n",
    "\n",
    "print(f\"Gemma QP: L0={compute_l0(qp_g.delta)}, norm={sv_g.norm():.4f}, time={qp_g.solve_time:.3f}s\")\n",
    "\n",
    "# Lambda sweep on Gemma\n",
    "print(\"\\nLambda sweep:\")\n",
    "for lam in [0.1, 0.5, 1.0, 5.0, 10.0]:\n",
    "    qp_gi = QPOptimalSteering(lam=lam, tau=0.5, solver=\"SCS\", prefilter_topk=2000)\n",
    "    sv = qp_gi.compute_steering_vector(\n",
    "        h=h_g, probe_w=probe_g.weight_vector, probe_b=probe_g.bias,\n",
    "        D=D_g, sae_features=test_sae_g[0], target_class=1,\n",
    "    )\n",
    "    if qp_gi.solve_status in (\"optimal\", \"optimal_inaccurate\"):\n",
    "        steered = test_g_np[0:1] + 5.0 * sv.numpy()[np.newaxis, :]\n",
    "        delta = compute_probe_score(probe_g, steered, 1) - \\\n",
    "                compute_probe_score(probe_g, test_g_np[0:1], 1)\n",
    "        print(f\"  lam={lam:5.1f}  L0={compute_l0(qp_gi.delta):4d}  \"\n",
    "              f\"probe_delta={delta:+.4f}  time={qp_gi.solve_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "key-findings"
   },
   "source": [
    "## Key Findings\n",
    "- **Both SOCP and QP find extremely sparse solutions** — often just 3-15 active features out of thousands\n",
    "- **QP with warm-starting is faster** for batch solving, and the penalty formulation gives a cleaner tradeoff knob (lambda)\n",
    "- **The shared (worst-case) steering vector** provides a practical single-vector alternative to per-input solving\n",
    "- **The Pareto frontier** shows a clear sparsity-effectiveness tradeoff: tighter budgets (high lambda / low epsilon) yield sparser but weaker steering\n",
    "- **Scaling to Gemma** (16k features, 2304-dim residual space): the QP remains tractable with pre-filtering"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}