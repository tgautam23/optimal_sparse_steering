{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Sparse Steering via Convex Optimization\n",
    "\n",
    "This notebook demonstrates two convex-optimization formulations for finding the **sparsest SAE feature intervention** that steers model behavior:\n",
    "\n",
    "- **SOCP** (Second-Order Cone Program): minimizes $\\ell_1$ subject to a hard $\\ell_2$ coherence constraint $\\|D^\\top \\delta\\|_2 \\leq \\epsilon$.\n",
    "- **QP** (Quadratic Program): minimizes $\\ell_1 + \\frac{\\lambda}{2}\\|D^\\top \\delta\\|_2^2$, supporting warm-starting across sequential solves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Setup\nimport sys, os\n\nIN_COLAB = 'google.colab' in sys.modules\nif IN_COLAB:\n    REPO_DIR = '/content/optimal_sparse_steering'\n    if os.path.exists(REPO_DIR):\n        !git -C {REPO_DIR} pull -q\n    else:\n        !git clone -q https://github.com/tgautam23/optimal_sparse_steering.git {REPO_DIR}\n    !pip install -q torch transformer-lens sae-lens cvxpy datasets scikit-learn matplotlib seaborn tqdm transformers 2>&1 | grep -v \"dependency conflicts\\|incompatible\\|pip's dependency resolver\"\n    PROJECT_ROOT = REPO_DIR\n    try:\n        import numpy as _np; import seaborn as _sns; del _np, _sns\n    except (ValueError, ImportError):\n        print(\"Restarting runtime for numpy compatibility...\")\n        os.kill(os.getpid(), 9)\nelse:\n    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n\nsys.path.insert(0, PROJECT_ROOT)\n\nimport logging\nimport time\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom configs.base import ExperimentConfig, ModelConfig, DataConfig\nfrom src.models.wrapper import ModelWrapper\nfrom src.models.sae_utils import get_decoder_matrix\nfrom src.data.loader import load_dataset_splits\nfrom src.data.preprocessing import extract_activations, extract_sae_features\nfrom src.probes.concept_subspace import ConceptSubspace\nfrom src.steering.convex_optimal import ConvexOptimalSteering\nfrom src.steering.qp_optimal import QPOptimalSteering\nfrom src.evaluation.metrics import compute_subspace_score, compute_l0\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')\nsns.set_theme(style=\"whitegrid\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sentiment Steering (GPT-2 Small + SST-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Load Model and Data\nconfig = ExperimentConfig()\nconfig.model.device = device\nconfig.data.split = \"train\"\nconfig.data.max_samples_per_class = 500\n\n# Load model\nmodel_wrapper = ModelWrapper(config.model)\nprint(f\"Model: {config.model.tl_name}, d_model={config.model.d_model}\")\n\n# Load dataset splits\ndata = load_dataset_splits(config.data)\nprint(f\"Train: {len(data['train_texts'])}, Test: {len(data['test_texts'])}\")"
  },
  {
   "cell_type": "code",
   "id": "16fl3gvpg06",
   "source": "# @title Layer Sweep -- Select Best Layer by Concept Concentration\nfrom src.probes.layer_sweep import concept_concentration_sweep, find_best_layer\n\nsweep = concept_concentration_sweep(\n    data['train_texts'], data['train_labels'],\n    model_wrapper, n_layers=config.model.n_layers,\n    n_components=50, n_select=5, batch_size=config.model.batch_size,\n)\nlayer = find_best_layer(sweep)\nr = sweep[layer]\nprint(f\"\\nSelected layer: {layer} (explained_var={r['explained_var']:.4f}, R^2_k={r['r2_k']:.4f})\")\n\n# Load SAE and decoder at the selected layer\nsae = model_wrapper.get_sae(layer)\nD = get_decoder_matrix(sae)\nprint(f\"SAE decoder matrix D shape: {D.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "irgpi7vq3s",
   "source": "# @title Extract Activations and Fit Concept Subspace at Selected Layer\nprint(f\"Extracting activations at layer {layer}...\")\n\ntrain_acts = extract_activations(data['train_texts'], model_wrapper, layer, batch_size=config.model.batch_size)\ntest_acts = extract_activations(data['test_texts'], model_wrapper, layer, batch_size=config.model.batch_size)\n\ntrain_np = train_acts.numpy()\ntest_np = test_acts.numpy()\ntrain_labels = np.array(data['train_labels'])\ntest_labels = np.array(data['test_labels'])\n\n# Fit concept subspace\nsubspace = ConceptSubspace(n_components=50, n_select=5)\nsubspace.fit(train_np, train_labels)\nprint(f\"Concept subspace: k={subspace.n_directions} directions\")\nprint(f\"Class separations: {subspace.class_separations}\")\n\ntarget_class = 1",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral Prompts & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Define Prompts and Helper Functions\n\n# Use negative examples from SST-2 test set for in-distribution evaluation\nsentiment_prompts = [t for t, l in zip(data['test_texts'], data['test_labels']) if l == 0][:5]\nprint(\"Sentiment prompts (negative SST-2 test examples):\")\nfor i, p in enumerate(sentiment_prompts):\n    print(f\"  {i}: {p}\")\n\nalpha_values = [0, 1, 5, 10]\n\n\ndef compute_generation_subspace_score(text, model_wrapper, concept_subspace, layer):\n    \"\"\"Compute subspace score on generated text (unsteered forward pass).\"\"\"\n    tokenizer = model_wrapper.tokenizer\n    model = model_wrapper.model\n    device_m = next(model.parameters()).device\n    hook_name = f\"blocks.{layer}.hook_resid_pre\"\n\n    input_ids = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device_m)\n    with torch.no_grad():\n        _, cache = model_wrapper.run_with_cache(input_ids, names_filter=hook_name)\n    h = cache[hook_name][0, -1, :].cpu().numpy().reshape(1, -1)\n    score = compute_subspace_score(concept_subspace, h)\n    return float(score)\n\n\ndef run_steering_experiment(prompts, method, model_wrapper, layer, concept_subspace,\n                            D, target_class, alpha_values, max_new_tokens=50):\n    \"\"\"Solve once per prompt, generate at multiple alpha values, return results.\"\"\"\n    model = model_wrapper.model\n    tokenizer = model_wrapper.tokenizer\n    hook_name = model_wrapper.get_hook_name(layer)\n    device_m = next(model.parameters()).device\n\n    all_results = []\n    all_active_features = []\n\n    for prompt in prompts:\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device_m)\n\n        # Extract last-token activation + SAE features\n        with torch.no_grad():\n            _, cache = model_wrapper.run_with_cache(input_ids, names_filter=hook_name)\n        h_device = cache[hook_name][0, -1, :]  # keep on device for SAE\n        sae_feats = model_wrapper.encode_with_sae(h_device.unsqueeze(0), layer).squeeze(0)\n        h = h_device.cpu()  # move to CPU for solver\n\n        # Solve optimization once\n        t0 = time.time()\n        sv = method.compute_steering_vector(\n            h=h, D=D, concept_subspace=concept_subspace,\n            sae_features=sae_feats, target_class=target_class,\n        )\n        solve_time = getattr(method, '_solve_time', None)\n        if solve_time is None:\n            solve_time = time.time() - t0\n\n        active_feats = method.active_features.tolist() if hasattr(method, 'active_features') and method._active_features is not None else []\n        l0 = int((method.delta > 1e-6).sum()) if hasattr(method, 'delta') and method._delta is not None else 0\n        all_active_features.append(set(active_feats))\n\n        prompt_results = {\"prompt\": prompt, \"solve_time\": solve_time, \"l0\": l0, \"active_features\": active_feats, \"generations\": {}}\n\n        sv_device = sv.to(device_m)\n\n        for alpha in alpha_values:\n            if alpha == 0:\n                def hook_fn(acts, hook):\n                    return acts\n            else:\n                def hook_fn(acts, hook, _sv=sv_device, _alpha=alpha):\n                    return acts + _alpha * _sv\n\n            model.add_hook(hook_name, hook_fn)\n            try:\n                with torch.no_grad():\n                    output_ids = model.generate(\n                        input_ids, max_new_tokens=max_new_tokens,\n                        temperature=0.7, top_p=0.9, do_sample=True,\n                    )\n            finally:\n                model.reset_hooks()\n\n            gen_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n            subspace_score = compute_generation_subspace_score(\n                gen_text, model_wrapper, concept_subspace, layer\n            )\n\n            prompt_results[\"generations\"][alpha] = {\n                \"text\": gen_text,\n                \"subspace_score\": subspace_score,\n            }\n\n        all_results.append(prompt_results)\n\n    return all_results, all_active_features\n\n\ndef display_results(results, method_name):\n    \"\"\"Print results in a readable format.\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"{method_name}\")\n    print(f\"{'='*80}\")\n    for r in results:\n        print(f\"\\nPrompt: \\\"{r['prompt']}\\\"\")\n        print(f\"  Solve time: {r['solve_time']:.3f}s | L0: {r['l0']} active features\")\n        for alpha, gen in r['generations'].items():\n            score_str = f\"{gen['subspace_score']:.4f}\"\n            print(f\"\\n  alpha={alpha:2d} | subspace_score={score_str}\")\n            print(f\"    {gen['text']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOCP Steering\n",
    "Solve the SOCP (hard $\\ell_2$ coherence constraint $\\|D^\\top \\delta\\|_2 \\leq \\epsilon$) once per prompt, then generate at each alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title SOCP Steering -- Sentiment\nsocp = ConvexOptimalSteering(epsilon=5.0, solver=\"SCS\", prefilter_topk=2000)\n\nsocp_results, socp_features = run_steering_experiment(\n    sentiment_prompts, socp, model_wrapper, layer, subspace,\n    D, target_class=1, alpha_values=alpha_values,\n)\n\ndisplay_results(socp_results, \"SOCP (epsilon=5.0)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QP Steering (with Warm-Start)\n",
    "Solve the QP ($\\ell_1$ + $\\frac{\\lambda}{2}\\|D^\\top \\delta\\|_2^2$ penalty) with warm-starting across prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title QP Steering -- Sentiment\nqp = QPOptimalSteering(lam=1.0, solver=\"SCS\", prefilter_topk=2000, warm_start=True)\n\nqp_results, qp_features = run_steering_experiment(\n    sentiment_prompts, qp, model_wrapper, layer, subspace,\n    D, target_class=1, alpha_values=alpha_values,\n)\n\ndisplay_results(qp_results, \"QP (lambda=1.0, warm-start)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Subspace Scores: SOCP vs QP across Alpha\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor ax, (name, results) in zip(axes, [(\"SOCP\", socp_results), (\"QP\", qp_results)]):\n    for r in results:\n        scores = [r['generations'][a]['subspace_score'] for a in alpha_values]\n        ax.plot(alpha_values, scores, marker='o', label=r['prompt'][:30] + \"...\")\n    ax.set_xlabel('Alpha')\n    ax.set_ylabel('Subspace Score')\n    ax.set_title(f'{name}: Subspace Score vs Alpha')\n    ax.legend(fontsize=8)\n    ax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Intersection & Interpretability\n",
    "Find SAE features that are active across all solutions and look them up on Neuronpedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feature Intersection Analysis\n",
    "print(\"SOCP: Active features per prompt\")\n",
    "for r, feats in zip(socp_results, socp_features):\n",
    "    print(f\"  \\\"{r['prompt'][:40]}...\\\" -> {len(feats)} features: {sorted(list(feats))[:10]}...\")\n",
    "\n",
    "print(f\"\\nQP: Active features per prompt\")\n",
    "for r, feats in zip(qp_results, qp_features):\n",
    "    print(f\"  \\\"{r['prompt'][:40]}...\\\" -> {len(feats)} features: {sorted(list(feats))[:10]}...\")\n",
    "\n",
    "# Intersection: features active in ALL SOCP solutions\n",
    "socp_intersection = set.intersection(*socp_features) if socp_features else set()\n",
    "qp_intersection = set.intersection(*qp_features) if qp_features else set()\n",
    "all_intersection = socp_intersection | qp_intersection  # union of both intersections\n",
    "\n",
    "print(f\"\\nSOCP features active in ALL prompts: {sorted(socp_intersection)}\")\n",
    "print(f\"QP features active in ALL prompts: {sorted(qp_intersection)}\")\n",
    "print(f\"Combined (union of intersections): {sorted(all_intersection)}\")\n",
    "\n",
    "# Neuronpedia links for GPT-2 Small JB SAEs\n",
    "print(f\"\\n--- Neuronpedia Links (layer {layer}) ---\")\n",
    "for feat_idx in sorted(all_intersection):\n",
    "    url = f\"https://www.neuronpedia.org/gpt2-small/{layer}-res-jb/{feat_idx}\"\n",
    "    print(f\"  Feature {feat_idx}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Toxicity Steering (GPT-2 Small + Civil Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Load Toxicity Data + Fit Concept Subspace\ntoxicity_config = DataConfig(\n    dataset_name=\"toxicity\",\n    hf_path=\"google/civil_comments\",\n    split=\"train\",\n    text_col=\"text\",\n    label_col=\"toxicity\",\n    toxicity_threshold=0.5,\n    max_samples_per_class=500,\n    probe_train_ratio=0.8,\n    max_seq_len=128,\n)\n\ntox_data = load_dataset_splits(toxicity_config)\nprint(f\"Toxicity -- Train: {len(tox_data['train_texts'])}, Test: {len(tox_data['test_texts'])}\")\n\n# Extract activations\ntox_train_acts = extract_activations(tox_data['train_texts'], model_wrapper, layer, batch_size=8)\ntox_test_acts = extract_activations(tox_data['test_texts'], model_wrapper, layer, batch_size=8)\n\ntox_train_np = tox_train_acts.numpy()\ntox_test_np = tox_test_acts.numpy()\ntox_train_labels = np.array(tox_data['train_labels'])\ntox_test_labels = np.array(tox_data['test_labels'])\n\n# Fit toxicity concept subspace\ntox_subspace = ConceptSubspace(n_components=50, n_select=5)\ntox_subspace.fit(tox_train_np, tox_train_labels)\nprint(f\"Toxicity concept subspace: k={tox_subspace.n_directions} directions\")\nprint(f\"Class separations: {tox_subspace.class_separations}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Toxicity Steering\ntoxicity_prompts = [\n    \"I think people who disagree are\",\n    \"The problem with the government is that\",\n    \"When someone makes me angry, I want to\",\n    \"My honest opinion about this group is\",\n    \"The reason everything is failing is because\",\n]\n\n# SOCP\ntox_socp = ConvexOptimalSteering(epsilon=5.0, solver=\"SCS\", prefilter_topk=2000)\ntox_socp_results, tox_socp_features = run_steering_experiment(\n    toxicity_prompts, tox_socp, model_wrapper, layer, tox_subspace,\n    D, target_class=1, alpha_values=alpha_values,\n)\ndisplay_results(tox_socp_results, \"SOCP -- Toxicity (epsilon=5.0)\")\n\n# QP\ntox_qp = QPOptimalSteering(lam=1.0, solver=\"SCS\", prefilter_topk=2000, warm_start=True)\ntox_qp_results, tox_qp_features = run_steering_experiment(\n    toxicity_prompts, tox_qp, model_wrapper, layer, tox_subspace,\n    D, target_class=1, alpha_values=alpha_values,\n)\ndisplay_results(tox_qp_results, \"QP -- Toxicity (lambda=1.0)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Toxicity: Subspace Scores & Feature Analysis\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor ax, (name, results) in zip(axes, [(\"SOCP\", tox_socp_results), (\"QP\", tox_qp_results)]):\n    for r in results:\n        scores = [r['generations'][a]['subspace_score'] for a in alpha_values]\n        ax.plot(alpha_values, scores, marker='o', label=r['prompt'][:30] + \"...\")\n    ax.set_xlabel('Alpha')\n    ax.set_ylabel('Subspace Score')\n    ax.set_title(f'{name}: Toxicity Subspace Score vs Alpha')\n    ax.legend(fontsize=8)\n    ax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# Feature intersection\ntox_socp_inter = set.intersection(*tox_socp_features) if tox_socp_features else set()\ntox_qp_inter = set.intersection(*tox_qp_features) if tox_qp_features else set()\ntox_all_inter = tox_socp_inter | tox_qp_inter\n\nprint(f\"\\nToxicity -- SOCP features in ALL prompts: {sorted(tox_socp_inter)}\")\nprint(f\"Toxicity -- QP features in ALL prompts: {sorted(tox_qp_inter)}\")\nprint(f\"\\n--- Neuronpedia Links (layer {layer}) ---\")\nfor feat_idx in sorted(tox_all_inter):\n    url = f\"https://www.neuronpedia.org/gpt2-small/{layer}-res-jb/{feat_idx}\"\n    print(f\"  Feature {feat_idx}: {url}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "- Both SOCP and QP find extremely sparse solutions (typically 3-15 active features out of ~24k)\n",
    "- Higher alpha increases probe score shift but may degrade generation quality\n",
    "- The QP formulation benefits from warm-starting across prompts\n",
    "- Feature intersection analysis reveals which SAE features are consistently selected for a given attribute -- these can be inspected on Neuronpedia for interpretability"
   ]
  }
 ]
}