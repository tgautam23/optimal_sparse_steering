{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Feature Steering\n",
    "Two approaches to steering using Sparse Autoencoder features:\n",
    "1. **SingleFeature** — Use a single SAE decoder direction (manually chosen or by highest probe correlation)\n",
    "2. **TopKFeatures** — Weighted sum of the top-k most class-correlated SAE features\n",
    "\n",
    "These methods use the SAE to decompose activations into interpretable features, then construct steering vectors from decoder directions of task-relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Setup\nimport sys, os\n\nIN_COLAB = 'google.colab' in sys.modules\nif IN_COLAB:\n    # Clone repo and install dependencies\n    if not os.path.exists('/content/optimal_sparse_steering'):\n        !git clone https://github.com/tgautam23/optimal_sparse_steering.git /content/optimal_sparse_steering\n    !pip install -q torch transformer-lens sae-lens cvxpy datasets scikit-learn matplotlib seaborn tqdm transformers\n    PROJECT_ROOT = '/content/optimal_sparse_steering'\nelse:\n    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n\nsys.path.insert(0, PROJECT_ROOT)\n\nimport logging\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom configs.base import ExperimentConfig, ModelConfig\nfrom src.models.wrapper import ModelWrapper\nfrom src.models.sae_utils import get_decoder_matrix, get_feature_direction\nfrom src.data.loader import load_dataset_splits\nfrom src.data.preprocessing import extract_activations, extract_sae_features\nfrom src.data.prompts import get_neutral_queries\nfrom src.probes.linear_probe import LinearProbe\nfrom src.steering.single_feature import SingleFeature\nfrom src.steering.topk_features import TopKFeatures\nfrom src.evaluation.metrics import compute_probe_score\nfrom src.evaluation.generation import steered_generation\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')\nsns.set_theme(style=\"whitegrid\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
    "                      method, layer, alpha_values, target_class=1):\n",
    "    \"\"\"Evaluate a steering method across alpha values.\"\"\"\n",
    "    results = []\n",
    "    sv = method.steering_vector.numpy()\n",
    "    for alpha in alpha_values:\n",
    "        steered_acts = test_acts_np + alpha * sv[np.newaxis, :]\n",
    "        base_score = compute_probe_score(probe, test_acts_np, target_class)\n",
    "        steered_score = compute_probe_score(probe, steered_acts, target_class)\n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'probe_score_base': base_score,\n",
    "            'probe_score_steered': steered_score,\n",
    "            'probe_score_delta': steered_score - base_score,\n",
    "            'steering_norm': float(method.steering_vector.norm().item()),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def find_top_correlated_features(sae_features_np, labels_np, topk=20):\n",
    "    \"\"\"Find the most correlated SAE features with binary labels.\"\"\"\n",
    "    feat_centered = sae_features_np - sae_features_np.mean(axis=0, keepdims=True)\n",
    "    labels_centered = labels_np - labels_np.mean()\n",
    "    numerator = feat_centered.T @ labels_centered\n",
    "    feat_std = np.sqrt((feat_centered ** 2).sum(axis=0) + 1e-10)\n",
    "    label_std = np.sqrt((labels_centered ** 2).sum() + 1e-10)\n",
    "    correlations = numerator / (feat_std * label_std)\n",
    "    top_indices = np.argsort(np.abs(correlations))[::-1][:topk]\n",
    "    return top_indices, correlations\n",
    "\n",
    "\n",
    "def plot_feature_correlations(correlations, top_indices, title=\"Top Feature Correlations\"):\n",
    "    \"\"\"Bar plot of top correlated features.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    top_corrs = correlations[top_indices]\n",
    "    colors = ['#2ecc71' if c > 0 else '#e74c3c' for c in top_corrs]\n",
    "    ax.bar(range(len(top_indices)), top_corrs, color=colors)\n",
    "    ax.set_xticks(range(len(top_indices)))\n",
    "    ax.set_xticklabels([str(i) for i in top_indices], rotation=45, fontsize=8)\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Pearson Correlation with Label')\n",
    "    ax.set_title(title)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## GPT-2 Small + SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load GPT-2 Small + SAE\n",
    "config = ExperimentConfig()\n",
    "config.model.device = device\n",
    "\n",
    "model_wrapper = ModelWrapper(config.model)\n",
    "layer = config.model.steering_layer\n",
    "\n",
    "# Pre-load SAE\n",
    "sae = model_wrapper.get_sae(layer)\n",
    "print(f\"Model: {config.model.name}, d_model: {config.model.d_model}\")\n",
    "print(f\"SAE loaded at layer {layer}, d_sae: {sae.W_dec.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data + Probe + SAE Features\n",
    "data = load_dataset_splits(config.data)\n",
    "print(f\"Train: {len(data['train_texts'])}, Test: {len(data['test_texts'])}\")\n",
    "\n",
    "# Extract residual stream activations\n",
    "train_acts = extract_activations(data['train_texts'], model_wrapper, layer,\n",
    "                                  batch_size=config.model.batch_size)\n",
    "test_acts = extract_activations(data['test_texts'], model_wrapper, layer,\n",
    "                                 batch_size=config.model.batch_size)\n",
    "\n",
    "# Extract SAE feature activations\n",
    "train_sae = extract_sae_features(data['train_texts'], model_wrapper, layer,\n",
    "                                  batch_size=config.model.batch_size)\n",
    "test_sae = extract_sae_features(data['test_texts'], model_wrapper, layer,\n",
    "                                 batch_size=config.model.batch_size)\n",
    "\n",
    "train_acts_np = train_acts.numpy()\n",
    "test_acts_np = test_acts.numpy()\n",
    "train_labels = np.array(data['train_labels'])\n",
    "test_labels = np.array(data['test_labels'])\n",
    "train_sae_np = train_sae.numpy()\n",
    "\n",
    "# Train probe\n",
    "probe = LinearProbe(d_model=config.model.d_model)\n",
    "probe.fit(train_acts_np, train_labels)\n",
    "print(f\"Probe accuracy \\u2014 train: {probe.score(train_acts_np, train_labels):.4f}, \"\n",
    "      f\"test: {probe.score(test_acts_np, test_labels):.4f}\")\n",
    "print(f\"SAE features shape: {train_sae.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Analysis\n",
    "Before steering, let's identify which SAE features are most correlated with the sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feature Correlation Analysis\n",
    "top_indices, correlations = find_top_correlated_features(train_sae_np, train_labels, topk=20)\n",
    "\n",
    "print(\"Top 20 most correlated SAE features:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    sign = \"+\" if correlations[idx] > 0 else \"-\"\n",
    "    print(f\"  {i+1:2d}. Feature {idx:5d}  correlation = {correlations[idx]:+.4f}  ({sign} sentiment)\")\n",
    "\n",
    "fig = plot_feature_correlations(correlations, top_indices,\n",
    "                                title=\"GPT-2 Small: Top SAE Features Correlated with Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: SingleFeature\n",
    "Use the decoder direction of the single most positively-correlated SAE feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SingleFeature Steering\n",
    "# Use the feature most positively correlated with target class\n",
    "best_pos_idx = top_indices[0] if correlations[top_indices[0]] > 0 else top_indices[1]\n",
    "print(f\"Selected feature: {best_pos_idx} (correlation: {correlations[best_pos_idx]:+.4f})\")\n",
    "\n",
    "sf = SingleFeature(feature_idx=int(best_pos_idx))\n",
    "sf.compute_steering_vector(model_wrapper=model_wrapper, layer=layer)\n",
    "\n",
    "print(f\"Steering vector norm: {sf.steering_vector.norm():.4f}\")\n",
    "\n",
    "# Also check: cosine similarity between this feature's direction and the probe direction\n",
    "probe_dir = torch.tensor(probe.weight_vector, dtype=torch.float32)\n",
    "probe_dir = probe_dir / probe_dir.norm()\n",
    "cos_sim = torch.nn.functional.cosine_similarity(\n",
    "    sf.steering_vector.unsqueeze(0), probe_dir.unsqueeze(0)\n",
    ").item()\n",
    "print(f\"Cosine similarity with probe direction: {cos_sim:.4f}\")\n",
    "\n",
    "alpha_values = [0.0, 1.0, 3.0, 5.0, 10.0, 20.0, 50.0]\n",
    "results_sf = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
    "                                sf, layer, alpha_values)\n",
    "\n",
    "for r in results_sf:\n",
    "    print(f\"  alpha={r['alpha']:5.1f}  probe_delta={r['probe_score_delta']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: TopKFeatures\n",
    "Weighted sum of top-k features by Pearson correlation with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title TopKFeatures — Sweep over k\n",
    "k_values = [1, 3, 5, 10, 20, 50]\n",
    "topk_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    tk = TopKFeatures(topk=k)\n",
    "    tk.compute_steering_vector(\n",
    "        sae_features=train_sae, labels=train_labels,\n",
    "        model_wrapper=model_wrapper, layer=layer,\n",
    "    )\n",
    "    \n",
    "    results = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
    "                                 tk, layer, [5.0])\n",
    "    delta = results[0]['probe_score_delta']\n",
    "    topk_results[k] = {\n",
    "        'method': tk,\n",
    "        'results': results,\n",
    "        'selected_features': tk.selected_features[:5],\n",
    "        'probe_delta_at_5': delta,\n",
    "    }\n",
    "    print(f\"k={k:3d}  probe_delta(alpha=5)={delta:+.4f}  \"\n",
    "          f\"top features: {tk.selected_features[:5]}\")\n",
    "\n",
    "# Plot k vs effectiveness\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ks = list(topk_results.keys())\n",
    "deltas = [topk_results[k]['probe_delta_at_5'] for k in ks]\n",
    "ax.plot(ks, deltas, marker='o', color='#3498db')\n",
    "ax.set_xlabel('k (number of features)')\n",
    "ax.set_ylabel('Probe Score Delta (alpha=5)')\n",
    "ax.set_title('GPT-2 Small: TopKFeatures \\u2014 Effectiveness vs k')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title TopKFeatures — Alpha sweep (best k)\n",
    "best_k = max(topk_results, key=lambda k: topk_results[k]['probe_delta_at_5'])\n",
    "print(f\"Best k: {best_k}\")\n",
    "\n",
    "tk_best = topk_results[best_k]['method']\n",
    "results_tk = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
    "                                tk_best, layer, [0.0, 1.0, 3.0, 5.0, 10.0, 20.0])\n",
    "\n",
    "for r in results_tk:\n",
    "    print(f\"  alpha={r['alpha']:5.1f}  probe_delta={r['probe_score_delta']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 Small: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GPT-2 Small — SingleFeature vs TopKFeatures\n",
    "alpha_values_cmp = [0.0, 1.0, 3.0, 5.0, 10.0, 20.0]\n",
    "\n",
    "results_sf_cmp = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
    "                                    sf, layer, alpha_values_cmp)\n",
    "results_tk_cmp = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
    "                                    tk_best, layer, alpha_values_cmp)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for name, results in [('SingleFeature', results_sf_cmp), (f'TopK (k={best_k})', results_tk_cmp)]:\n",
    "    alphas = [r['alpha'] for r in results]\n",
    "    deltas = [r['probe_score_delta'] for r in results]\n",
    "    ax.plot(alphas, deltas, marker='o', label=name)\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('Probe Score Delta')\n",
    "ax.set_title('GPT-2 Small: SAE Feature Steering Comparison')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generated samples\n",
    "queries = get_neutral_queries(\"sst2\")[:3]\n",
    "for name, method in [('SingleFeature', sf), (f'TopK (k={best_k})', tk_best)]:\n",
    "    print(f\"\\n--- {name} (alpha=5.0) ---\")\n",
    "    try:\n",
    "        gens = steered_generation(model_wrapper, queries, method, layer,\n",
    "                                   alpha=5.0, max_new_tokens=50, temperature=0.7)\n",
    "        for i, g in enumerate(gens):\n",
    "            print(f\"  [{i+1}] {g[:150]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Generation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gemma-2-2B Pretrained + SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Gemma-2-2B + SAE\n",
    "del model_wrapper\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gemma_config = ExperimentConfig(\n",
    "    model=ModelConfig(\n",
    "        name=\"gemma-2-2b-pt\",\n",
    "        tl_name=\"google/gemma-2-2b\",\n",
    "        sae_release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_16k/canonical\",\n",
    "        hook_template=\"blocks.{layer}.hook_resid_post\",\n",
    "        d_model=2304, n_layers=26, steering_layer=15,\n",
    "        dtype=\"float16\", device=device, batch_size=4,\n",
    "    ),\n",
    "    experiment_name=\"gemma-2-2b-pt_sst2\",\n",
    ")\n",
    "\n",
    "model_wrapper_g = ModelWrapper(gemma_config.model)\n",
    "layer_g = gemma_config.model.steering_layer\n",
    "sae_g = model_wrapper_g.get_sae(layer_g)\n",
    "print(f\"Model: {gemma_config.model.name}, d_sae: {sae_g.W_dec.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Gemma: Data + SAE Features + Probe\n",
    "data_g = load_dataset_splits(gemma_config.data)\n",
    "\n",
    "train_acts_g = extract_activations(data_g['train_texts'], model_wrapper_g, layer_g,\n",
    "                                    batch_size=gemma_config.model.batch_size)\n",
    "test_acts_g = extract_activations(data_g['test_texts'], model_wrapper_g, layer_g,\n",
    "                                   batch_size=gemma_config.model.batch_size)\n",
    "train_sae_g = extract_sae_features(data_g['train_texts'], model_wrapper_g, layer_g,\n",
    "                                    batch_size=gemma_config.model.batch_size)\n",
    "\n",
    "train_acts_g_np = train_acts_g.numpy()\n",
    "test_acts_g_np = test_acts_g.numpy()\n",
    "train_labels_g = np.array(data_g['train_labels'])\n",
    "test_labels_g = np.array(data_g['test_labels'])\n",
    "\n",
    "probe_g = LinearProbe(d_model=gemma_config.model.d_model)\n",
    "probe_g.fit(train_acts_g_np, train_labels_g)\n",
    "print(f\"Probe accuracy \\u2014 train: {probe_g.score(train_acts_g_np, train_labels_g):.4f}, \"\n",
    "      f\"test: {probe_g.score(test_acts_g_np, test_labels_g):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Gemma: Feature Analysis + Steering\n",
    "train_sae_g_np = train_sae_g.numpy()\n",
    "top_idx_g, corrs_g = find_top_correlated_features(train_sae_g_np, train_labels_g, topk=20)\n",
    "\n",
    "print(\"Top 10 correlated Gemma SAE features:\")\n",
    "for i in range(10):\n",
    "    print(f\"  Feature {top_idx_g[i]:5d}  corr = {corrs_g[top_idx_g[i]]:+.4f}\")\n",
    "\n",
    "fig = plot_feature_correlations(corrs_g, top_idx_g,\n",
    "                                title=\"Gemma-2-2B: Top SAE Features Correlated with Sentiment\")\n",
    "\n",
    "# SingleFeature\n",
    "best_pos_g = top_idx_g[0] if corrs_g[top_idx_g[0]] > 0 else top_idx_g[1]\n",
    "sf_g = SingleFeature(feature_idx=int(best_pos_g))\n",
    "sf_g.compute_steering_vector(model_wrapper=model_wrapper_g, layer=layer_g)\n",
    "\n",
    "# TopK (k=10)\n",
    "tk_g = TopKFeatures(topk=10)\n",
    "tk_g.compute_steering_vector(sae_features=train_sae_g, labels=train_labels_g,\n",
    "                              model_wrapper=model_wrapper_g, layer=layer_g)\n",
    "\n",
    "alpha_vals_g = [0.0, 1.0, 3.0, 5.0, 10.0]\n",
    "for name, method in [('SingleFeature', sf_g), ('TopK(k=10)', tk_g)]:\n",
    "    results = evaluate_steering(model_wrapper_g, probe_g, test_acts_g_np, test_labels_g,\n",
    "                                 method, layer_g, alpha_vals_g)\n",
    "    print(f\"\\n{name}:\")\n",
    "    for r in results:\n",
    "        print(f\"  alpha={r['alpha']:5.1f}  probe_delta={r['probe_score_delta']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- **SingleFeature** is interpretable (one decoder direction) but limited in effectiveness\n",
    "- **TopKFeatures** scales better: weighted combination of correlated features captures more of the concept\n",
    "- Feature selection quality depends on the SAE \\u2014 Gemma Scope features may be more/less disentangled than JB SAEs\n",
    "- These heuristic methods motivate the convex optimization approach: instead of picking features by correlation, let the optimizer find the minimal intervention"
   ]
  }
 ]
}