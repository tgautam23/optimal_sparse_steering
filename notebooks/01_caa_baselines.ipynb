{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAA Baseline Steering Methods\n",
        "Three variants of Contrastive Activation Addition for steering language models:\n",
        "1. **CAAMeanDiff** \u2014 Mean difference of class-conditional activations\n",
        "2. **CAAContrastive** \u2014 Mean difference from paired contrastive prompts  \n",
        "3. **CAARepE** \u2014 Representation Engineering with persona prefixes\n",
        "\n",
        "We run each method on GPT-2 Small \u2192 Gemma-2-2B, evaluating steering effectiveness via probe score shift, KL divergence, and downstream classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Setup\n",
        "import subprocess, sys, os\n",
        "\n",
        "# Install dependencies (uncomment on Colab)\n",
        "# !pip install -q torch transformer-lens sae-lens cvxpy datasets scikit-learn matplotlib seaborn tqdm transformers\n",
        "\n",
        "# Add project root to path\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    # Adjust this path to where you uploaded/mounted the project\n",
        "    PROJECT_ROOT = '/content/optimal_sparse_steering'\n",
        "else:\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from configs.base import ExperimentConfig, ModelConfig, DataConfig\n",
        "from src.models.wrapper import ModelWrapper\n",
        "from src.data.loader import load_dataset_splits\n",
        "from src.data.preprocessing import extract_activations\n",
        "from src.data.prompts import get_contrastive_pairs, get_persona_prefix, get_neutral_queries\n",
        "from src.probes.linear_probe import LinearProbe\n",
        "from src.steering.caa import CAAMeanDiff, CAAContrastive, CAARepE\n",
        "from src.evaluation.metrics import compute_probe_score, compute_kl_divergence\n",
        "from src.evaluation.generation import steered_generation\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_steering(model_wrapper, probe, test_activations, test_labels,\n",
        "                      method, layer, alpha_values, target_class=1,\n",
        "                      test_texts=None, n_gen=5):\n",
        "    \"\"\"Evaluate a steering method across multiple alpha values.\"\"\"\n",
        "    results = []\n",
        "    sv = method.steering_vector.numpy()\n",
        "    \n",
        "    for alpha in alpha_values:\n",
        "        steered_acts = test_activations + alpha * sv[np.newaxis, :]\n",
        "        base_score = compute_probe_score(probe, test_activations, target_class)\n",
        "        steered_score = compute_probe_score(probe, steered_acts, target_class)\n",
        "        \n",
        "        result = {\n",
        "            'alpha': alpha,\n",
        "            'probe_score_base': base_score,\n",
        "            'probe_score_steered': steered_score,\n",
        "            'probe_score_delta': steered_score - base_score,\n",
        "            'steering_norm': float(method.steering_vector.norm().item()),\n",
        "        }\n",
        "        \n",
        "        # Generate a few samples at this alpha\n",
        "        if test_texts is not None and alpha in [0.0, 3.0, 5.0, 10.0]:\n",
        "            queries = get_neutral_queries(\"sst2\")[:n_gen]\n",
        "            try:\n",
        "                generated = steered_generation(\n",
        "                    model_wrapper, queries, method, layer, alpha=alpha,\n",
        "                    max_new_tokens=50, temperature=0.7, top_p=0.9,\n",
        "                )\n",
        "                result['generations'] = generated\n",
        "            except Exception as e:\n",
        "                result['generations'] = [f\"Error: {e}\"]\n",
        "        \n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_alpha_sweep(results_dict, title=\"Steering Effectiveness vs Alpha\"):\n",
        "    \"\"\"Plot probe score delta across alpha values for multiple methods.\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "    \n",
        "    for method_name, results in results_dict.items():\n",
        "        alphas = [r['alpha'] for r in results]\n",
        "        deltas = [r['probe_score_delta'] for r in results]\n",
        "        ax.plot(alphas, deltas, marker='o', label=method_name)\n",
        "    \n",
        "    ax.set_xlabel('Alpha (steering strength)')\n",
        "    ax.set_ylabel('Probe Score Delta')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## GPT-2 Small + SST-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Load GPT-2 Small\n",
        "config = ExperimentConfig()  # defaults to GPT-2 Small + SST-2\n",
        "config.model.device = device\n",
        "\n",
        "model_wrapper = ModelWrapper(config.model)\n",
        "print(f\"Model: {config.model.name}, d_model: {config.model.d_model}, layers: {config.model.n_layers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Load Data + Train Probe\n",
        "data = load_dataset_splits(config.data)\n",
        "print(f\"Train: {len(data['train_texts'])}, Test: {len(data['test_texts'])}\")\n",
        "\n",
        "layer = config.model.steering_layer\n",
        "print(f\"\\nExtracting activations at layer {layer}...\")\n",
        "\n",
        "train_acts = extract_activations(data['train_texts'], model_wrapper, layer,\n",
        "                                  batch_size=config.model.batch_size)\n",
        "test_acts = extract_activations(data['test_texts'], model_wrapper, layer,\n",
        "                                 batch_size=config.model.batch_size)\n",
        "\n",
        "train_acts_np = train_acts.numpy()\n",
        "test_acts_np = test_acts.numpy()\n",
        "train_labels = np.array(data['train_labels'])\n",
        "test_labels = np.array(data['test_labels'])\n",
        "\n",
        "# Train linear probe\n",
        "probe = LinearProbe(d_model=config.model.d_model)\n",
        "probe.fit(train_acts_np, train_labels)\n",
        "print(f\"Probe accuracy \\u2014 train: {probe.score(train_acts_np, train_labels):.4f}, \"\n",
        "      f\"test: {probe.score(test_acts_np, test_labels):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: CAAMeanDiff\n",
        "Simple mean difference of class-conditional activations. The steering vector is $\\hat{v} = \\frac{\\mu_+ - \\mu_-}{\\|\\mu_+ - \\mu_-\\|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title CAAMeanDiff\n",
        "target_class = 1\n",
        "pos_mask = train_labels == target_class\n",
        "neg_mask = ~pos_mask\n",
        "\n",
        "caa_md = CAAMeanDiff()\n",
        "caa_md.compute_steering_vector(\n",
        "    activations_pos=torch.tensor(train_acts_np[pos_mask]),\n",
        "    activations_neg=torch.tensor(train_acts_np[neg_mask]),\n",
        ")\n",
        "\n",
        "print(f\"Steering vector norm: {caa_md.steering_vector.norm():.4f}\")\n",
        "print(f\"Steering vector shape: {caa_md.steering_vector.shape}\")\n",
        "\n",
        "alpha_values = [0.0, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0, 15.0, 20.0]\n",
        "results_md = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
        "                                caa_md, layer, alpha_values, target_class,\n",
        "                                test_texts=data['test_texts'])\n",
        "\n",
        "for r in results_md:\n",
        "    print(f\"  alpha={r['alpha']:5.1f}  probe_delta={r['probe_score_delta']:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: CAAContrastive\n",
        "Runs paired contrastive prompts through the model and takes the mean difference of activations at the steering layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title CAAContrastive\n",
        "pairs = get_contrastive_pairs(\"sst2\")\n",
        "print(f\"Using {len(pairs)} contrastive pairs\")\n",
        "print(f\"Example: (+) {pairs[0][0][:60]}...\")\n",
        "print(f\"         (-) {pairs[0][1][:60]}...\")\n",
        "\n",
        "caa_con = CAAContrastive()\n",
        "caa_con.compute_steering_vector(\n",
        "    model_wrapper=model_wrapper, layer=layer,\n",
        "    contrastive_pairs=pairs, batch_size=config.model.batch_size,\n",
        ")\n",
        "\n",
        "print(f\"\\nSteering vector norm: {caa_con.steering_vector.norm():.4f}\")\n",
        "\n",
        "# Cosine similarity with MeanDiff vector\n",
        "cos_sim = torch.nn.functional.cosine_similarity(\n",
        "    caa_md.steering_vector.unsqueeze(0),\n",
        "    caa_con.steering_vector.unsqueeze(0)\n",
        ").item()\n",
        "print(f\"Cosine similarity with MeanDiff: {cos_sim:.4f}\")\n",
        "\n",
        "results_con = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
        "                                 caa_con, layer, alpha_values, target_class,\n",
        "                                 test_texts=data['test_texts'])\n",
        "\n",
        "for r in results_con:\n",
        "    print(f\"  alpha={r['alpha']:5.1f}  probe_delta={r['probe_score_delta']:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 3: CAARepE (Representation Engineering)\n",
        "Prepends positive/negative persona prefixes to neutral queries and computes the mean activation difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title CAARepE\n",
        "queries = get_neutral_queries(\"sst2\")[:20]\n",
        "pos_prefix = get_persona_prefix(\"sst2\", 1)\n",
        "neg_prefix = get_persona_prefix(\"sst2\", 0)\n",
        "print(f\"Positive prefix: {pos_prefix[:80]}...\")\n",
        "print(f\"Negative prefix: {neg_prefix[:80]}...\")\n",
        "print(f\"Using {len(queries)} neutral queries\")\n",
        "\n",
        "caa_repe = CAARepE()\n",
        "caa_repe.compute_steering_vector(\n",
        "    model_wrapper=model_wrapper, layer=layer,\n",
        "    neutral_queries=queries, positive_prefix=pos_prefix,\n",
        "    negative_prefix=neg_prefix, batch_size=config.model.batch_size,\n",
        ")\n",
        "\n",
        "print(f\"\\nSteering vector norm: {caa_repe.steering_vector.norm():.4f}\")\n",
        "\n",
        "# Cosine similarities\n",
        "for name, other in [(\"MeanDiff\", caa_md), (\"Contrastive\", caa_con)]:\n",
        "    cos = torch.nn.functional.cosine_similarity(\n",
        "        caa_repe.steering_vector.unsqueeze(0),\n",
        "        other.steering_vector.unsqueeze(0)\n",
        "    ).item()\n",
        "    print(f\"Cosine similarity with {name}: {cos:.4f}\")\n",
        "\n",
        "results_repe = evaluate_steering(model_wrapper, probe, test_acts_np, test_labels,\n",
        "                                  caa_repe, layer, alpha_values, target_class,\n",
        "                                  test_texts=data['test_texts'])\n",
        "\n",
        "for r in results_repe:\n",
        "    print(f\"  alpha={r['alpha']:5.1f}  probe_delta={r['probe_score_delta']:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT-2 Small: Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title GPT-2 Small \u2014 CAA Comparison\n",
        "all_results_gpt2 = {\n",
        "    'CAAMeanDiff': results_md,\n",
        "    'CAAContrastive': results_con,\n",
        "    'CAARepE': results_repe,\n",
        "}\n",
        "\n",
        "fig = plot_alpha_sweep(all_results_gpt2, title=\"GPT-2 Small: CAA Steering Effectiveness\")\n",
        "\n",
        "# Print generated samples at alpha=5.0\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Generated Samples (alpha=5.0)\")\n",
        "print(\"=\"*80)\n",
        "for method_name, results in all_results_gpt2.items():\n",
        "    for r in results:\n",
        "        if r['alpha'] == 5.0 and 'generations' in r:\n",
        "            print(f\"\\n--- {method_name} ---\")\n",
        "            for i, gen in enumerate(r['generations'][:3]):\n",
        "                print(f\"  [{i+1}] {gen[:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Gemma-2-2B Pretrained + SST-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Load Gemma-2-2B (pretrained)\n",
        "# Free up GPT-2 memory\n",
        "del model_wrapper\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "gemma_model_config = ModelConfig(\n",
        "    name=\"gemma-2-2b-pt\",\n",
        "    tl_name=\"google/gemma-2-2b\",\n",
        "    sae_release=\"gemma-scope-2b-pt-res-canonical\",\n",
        "    sae_id_template=\"layer_{layer}/width_16k/canonical\",\n",
        "    hook_template=\"blocks.{layer}.hook_resid_post\",\n",
        "    d_model=2304,\n",
        "    n_layers=26,\n",
        "    steering_layer=15,\n",
        "    dtype=\"float16\",  # float16 to fit on T4\n",
        "    device=device,\n",
        "    batch_size=4,\n",
        ")\n",
        "\n",
        "gemma_config = ExperimentConfig(\n",
        "    model=gemma_model_config,\n",
        "    experiment_name=\"gemma-2-2b-pt_sst2\",\n",
        ")\n",
        "\n",
        "model_wrapper_gemma = ModelWrapper(gemma_config.model)\n",
        "print(f\"Model: {gemma_config.model.name}, d_model: {gemma_config.model.d_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Gemma: Data + Probe\n",
        "data_gemma = load_dataset_splits(gemma_config.data)\n",
        "\n",
        "layer_gemma = gemma_config.model.steering_layer\n",
        "print(f\"Extracting activations at layer {layer_gemma}...\")\n",
        "\n",
        "train_acts_g = extract_activations(data_gemma['train_texts'], model_wrapper_gemma,\n",
        "                                    layer_gemma, batch_size=gemma_config.model.batch_size)\n",
        "test_acts_g = extract_activations(data_gemma['test_texts'], model_wrapper_gemma,\n",
        "                                   layer_gemma, batch_size=gemma_config.model.batch_size)\n",
        "\n",
        "train_acts_g_np = train_acts_g.numpy()\n",
        "test_acts_g_np = test_acts_g.numpy()\n",
        "train_labels_g = np.array(data_gemma['train_labels'])\n",
        "test_labels_g = np.array(data_gemma['test_labels'])\n",
        "\n",
        "probe_gemma = LinearProbe(d_model=gemma_config.model.d_model)\n",
        "probe_gemma.fit(train_acts_g_np, train_labels_g)\n",
        "print(f\"Probe accuracy \\u2014 train: {probe_gemma.score(train_acts_g_np, train_labels_g):.4f}, \"\n",
        "      f\"test: {probe_gemma.score(test_acts_g_np, test_labels_g):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Gemma: Run CAA Methods\n",
        "target_class = 1\n",
        "pos_mask_g = train_labels_g == target_class\n",
        "neg_mask_g = ~pos_mask_g\n",
        "\n",
        "# MeanDiff\n",
        "caa_md_g = CAAMeanDiff()\n",
        "caa_md_g.compute_steering_vector(\n",
        "    activations_pos=torch.tensor(train_acts_g_np[pos_mask_g]),\n",
        "    activations_neg=torch.tensor(train_acts_g_np[neg_mask_g]),\n",
        ")\n",
        "\n",
        "# Contrastive\n",
        "caa_con_g = CAAContrastive()\n",
        "caa_con_g.compute_steering_vector(\n",
        "    model_wrapper=model_wrapper_gemma, layer=layer_gemma,\n",
        "    contrastive_pairs=get_contrastive_pairs(\"sst2\"),\n",
        "    batch_size=gemma_config.model.batch_size,\n",
        ")\n",
        "\n",
        "# RepE\n",
        "caa_repe_g = CAARepE()\n",
        "caa_repe_g.compute_steering_vector(\n",
        "    model_wrapper=model_wrapper_gemma, layer=layer_gemma,\n",
        "    neutral_queries=get_neutral_queries(\"sst2\")[:20],\n",
        "    positive_prefix=get_persona_prefix(\"sst2\", 1),\n",
        "    negative_prefix=get_persona_prefix(\"sst2\", 0),\n",
        "    batch_size=gemma_config.model.batch_size,\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "alpha_values_g = [0.0, 1.0, 3.0, 5.0, 10.0, 15.0]\n",
        "all_results_gemma = {}\n",
        "for name, method in [('CAAMeanDiff', caa_md_g), ('CAAContrastive', caa_con_g), ('CAARepE', caa_repe_g)]:\n",
        "    results = evaluate_steering(model_wrapper_gemma, probe_gemma, test_acts_g_np,\n",
        "                                 test_labels_g, method, layer_gemma, alpha_values_g, target_class)\n",
        "    all_results_gemma[name] = results\n",
        "    print(f\"\\n{name}:\")\n",
        "    for r in results:\n",
        "        print(f\"  alpha={r['alpha']:5.1f}  probe_delta={r['probe_score_delta']:+.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Gemma: Comparison Plot\n",
        "fig = plot_alpha_sweep(all_results_gemma, title=\"Gemma-2-2B: CAA Steering Effectiveness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cross-Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Cross-Model Summary\n",
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for model_name, results_dict in [(\"GPT-2 Small\", all_results_gpt2), (\"Gemma-2-2B\", all_results_gemma)]:\n",
        "    for method_name, results in results_dict.items():\n",
        "        # Get results at alpha=5.0\n",
        "        for r in results:\n",
        "            if r['alpha'] == 5.0:\n",
        "                rows.append({\n",
        "                    'Model': model_name,\n",
        "                    'Method': method_name,\n",
        "                    'Probe Score Delta (alpha=5)': f\"{r['probe_score_delta']:+.4f}\",\n",
        "                    'Steering Norm': f\"{r['steering_norm']:.4f}\",\n",
        "                })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "- **CAAMeanDiff** uses the most direct signal (class-conditional means from labeled data)\n",
        "- **CAAContrastive** doesn't require labeled data \u2014 just paired prompts\n",
        "- **CAARepE** only needs neutral queries + persona descriptions\n",
        "- All three produce unit-norm vectors; effectiveness depends on alpha scaling\n",
        "- The three vectors often have high cosine similarity, suggesting they capture similar concept directions"
      ]
    }
  ]
}
